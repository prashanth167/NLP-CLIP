{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.2682859003543854\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name=\"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Load the model and processor and move the model to GPU\n",
    "model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Load your image and text\n",
    "image = Image.open(\"Untitled.jpg\")  # Replace with your image path\n",
    "text = \"dog\"  # Replace with the text description\n",
    "\n",
    "# Process inputs and move to GPU\n",
    "inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to GPU\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(inputs[\"pixel_values\"])\n",
    "    text_features = model.get_text_features(inputs[\"input_ids\"])\n",
    "\n",
    "# Normalize features\n",
    "image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity = (image_features @ text_features.T).item()\n",
    "print(\"Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Embeddings: tensor([[ 3.1283e-02,  5.2002e-03,  3.3402e-03, -5.5785e-03,  8.5385e-03,\n",
      "         -6.9483e-02,  2.1888e-02,  3.6923e-02, -2.6185e-02, -8.1141e-03,\n",
      "         -3.3083e-02, -1.4145e-02,  3.5007e-02,  3.9772e-02,  5.5885e-02,\n",
      "          1.5132e-02,  4.8115e-02, -1.4221e-02,  1.7044e-02, -1.1784e-02,\n",
      "         -5.3425e-02,  3.4998e-03,  1.8184e-02, -4.4850e-02, -2.7819e-02,\n",
      "         -4.2605e-03,  3.0013e-02,  4.0829e-03,  1.0472e-02, -1.5174e-02,\n",
      "          1.5670e-02,  1.2465e-02, -2.8104e-02,  4.5190e-04,  8.1788e-03,\n",
      "          4.4739e-02,  2.7242e-02,  2.4861e-02, -2.3063e-02,  5.6737e-02,\n",
      "         -5.1856e-02,  2.2394e-02, -1.2018e-02, -4.7851e-03,  2.3800e-02,\n",
      "          5.3726e-02, -2.5420e-02, -1.8690e-03,  1.6551e-02,  2.1403e-02,\n",
      "          1.1667e-02,  3.2170e-02, -9.3857e-03, -3.0882e-02,  3.9441e-02,\n",
      "          2.2059e-03, -5.5880e-03,  1.9732e-03, -1.8575e-02, -1.7405e-03,\n",
      "          1.3367e-01,  1.9937e-02,  2.9486e-02,  2.1308e-02, -2.3493e-02,\n",
      "         -2.2152e-02, -1.9999e-02,  4.4453e-02,  5.1424e-03,  2.1006e-03,\n",
      "         -1.6449e-02,  1.8930e-03, -1.6433e-02, -1.0450e-03, -6.7937e-02,\n",
      "         -1.0675e-02,  2.5818e-03, -1.7340e-02,  1.2334e-02, -5.1722e-03,\n",
      "          1.8006e-02, -1.3102e-02, -8.4444e-03, -5.1290e-02,  4.6529e-02,\n",
      "         -9.8118e-03,  2.2515e-02, -2.5616e-02,  2.0881e-02, -5.7120e-03,\n",
      "         -2.4662e-02, -1.8288e-02, -6.9394e-01, -1.5835e-02, -2.0917e-02,\n",
      "         -2.8843e-03, -1.3378e-03,  1.9028e-02, -1.2042e-01,  7.2267e-04,\n",
      "          1.0743e-02, -6.6425e-03,  2.6124e-02,  2.1205e-02,  4.6964e-03,\n",
      "          5.6861e-03,  1.4194e-01,  3.5812e-02,  1.7656e-02, -6.2686e-03,\n",
      "          2.9213e-02,  4.1378e-02,  3.5151e-02, -1.1527e-02,  2.7056e-03,\n",
      "          3.1676e-02, -1.2973e-02, -6.4829e-03, -4.6927e-03, -1.8937e-02,\n",
      "          2.2851e-02,  8.0811e-02,  3.1995e-02, -2.8464e-02,  3.3883e-03,\n",
      "         -2.8388e-02, -2.0240e-03,  2.7219e-02, -7.7468e-03, -4.5285e-02,\n",
      "          1.9688e-02,  3.5230e-02, -3.4991e-02,  8.8111e-02, -5.1959e-03,\n",
      "         -4.3979e-03, -8.9194e-03, -5.9982e-02,  2.4886e-02, -2.3279e-02,\n",
      "          1.1411e-02,  1.0097e-03, -6.1988e-02,  3.1051e-02, -7.1367e-02,\n",
      "          6.1366e-02, -1.8931e-03,  2.6906e-02, -2.1772e-02,  2.2799e-02,\n",
      "         -7.0084e-03,  2.5087e-02,  1.7243e-02, -1.4706e-02, -5.0919e-02,\n",
      "          2.5478e-02, -4.3417e-02, -4.2435e-02, -4.7437e-02,  1.5383e-02,\n",
      "         -7.2649e-03,  2.8463e-02, -1.5253e-02, -2.9901e-03,  6.6340e-02,\n",
      "         -6.7421e-04, -2.7199e-02, -8.3017e-03, -1.3197e-02,  4.1340e-02,\n",
      "         -1.4244e-02,  5.2710e-03,  2.5471e-02, -1.6106e-02,  2.2776e-02,\n",
      "          2.3877e-02,  1.3321e-03, -3.9525e-02,  5.0993e-02, -2.9953e-03,\n",
      "         -3.1228e-02,  2.1697e-02,  1.7605e-02, -2.3655e-02,  1.0825e-02,\n",
      "         -1.0513e-02, -5.2213e-02,  2.5015e-02,  1.2658e-03,  4.5068e-03,\n",
      "          1.6089e-02,  1.8044e-02,  3.0003e-02, -2.6407e-02, -5.0830e-02,\n",
      "          2.0506e-02,  1.3706e-02, -5.7185e-03, -7.7445e-02, -2.2642e-03,\n",
      "          2.3690e-03, -2.0729e-02,  2.1973e-02,  2.2604e-02, -3.8024e-02,\n",
      "         -2.6099e-02,  2.9076e-03, -3.2545e-02, -5.2962e-03,  3.7460e-02,\n",
      "          2.8629e-02,  1.2990e-02,  1.1609e-02,  3.4629e-02, -2.8732e-02,\n",
      "          2.0967e-03,  5.7943e-03,  2.7685e-02,  1.4918e-01, -5.8148e-03,\n",
      "          7.9054e-02,  1.7404e-02,  4.2343e-02,  3.2010e-02, -3.8215e-02,\n",
      "         -2.1082e-02,  1.2281e-02, -1.5966e-02, -5.6317e-03, -1.8966e-02,\n",
      "          1.5730e-02,  2.5231e-02,  1.5966e-02,  7.4476e-02,  1.6271e-02,\n",
      "         -2.8937e-02, -1.5472e-02, -4.9587e-02,  6.0932e-02, -1.6582e-02,\n",
      "          6.2420e-03,  3.2814e-02,  1.0201e-02, -5.5910e-03,  3.9067e-02,\n",
      "          4.0080e-02,  6.8649e-03, -6.5012e-02,  3.8244e-02,  8.9345e-03,\n",
      "          3.3840e-02, -4.5466e-03,  2.3553e-02, -3.8270e-02,  3.8003e-03,\n",
      "          1.3332e-02,  6.2641e-03,  2.3314e-02,  6.7997e-03, -1.4600e-02,\n",
      "         -2.9341e-02,  1.6868e-02,  2.9968e-02, -1.5667e-02, -1.9940e-02,\n",
      "          5.8254e-04, -3.0615e-02, -2.4846e-02, -3.7043e-02,  4.5600e-02,\n",
      "         -8.6389e-04,  5.8678e-03,  2.7811e-02,  1.7025e-02, -2.1353e-02,\n",
      "         -9.2863e-03,  1.6676e-02,  3.3445e-02, -1.1950e-03,  1.9058e-02,\n",
      "         -2.4881e-02, -2.7250e-03, -3.6113e-02,  3.6983e-03, -4.9613e-03,\n",
      "         -1.2403e-02, -5.9630e-02, -2.7039e-02,  6.3973e-03,  6.8911e-03,\n",
      "          5.7902e-03,  2.5568e-02,  5.0098e-03,  2.4740e-02, -3.7524e-03,\n",
      "          2.2449e-02, -8.0686e-03, -1.1064e-03,  2.5422e-02, -3.8161e-02,\n",
      "         -5.2728e-04, -9.8043e-03, -5.0635e-02, -1.3853e-02,  1.6933e-02,\n",
      "          3.0517e-02, -4.6225e-03,  2.2734e-02,  1.4645e-02,  3.6421e-02,\n",
      "         -3.2006e-03,  1.3439e-02,  8.8013e-02, -2.9636e-02, -4.7638e-03,\n",
      "         -3.5015e-02,  1.5948e-02,  2.9863e-02, -5.0690e-02, -5.5794e-02,\n",
      "          5.0995e-02,  7.0018e-02, -1.0750e-02,  1.1484e-02,  2.5384e-02,\n",
      "          1.3387e-02, -2.2882e-02,  2.2814e-02,  1.1725e-02,  3.4568e-02,\n",
      "         -8.6190e-03, -4.2205e-02,  1.1417e-02, -1.9172e-02,  9.2990e-03,\n",
      "          2.2936e-03,  4.5281e-02,  2.0603e-02, -2.5039e-02, -5.9579e-02,\n",
      "         -9.2391e-03, -1.1338e-02, -1.4551e-02, -6.2891e-03,  1.0657e-02,\n",
      "          3.3063e-02,  1.7793e-02,  1.2594e-02, -7.6607e-03, -2.6913e-02,\n",
      "         -8.1131e-02,  2.5462e-02,  3.3178e-02,  1.1858e-02,  1.3039e-02,\n",
      "          5.8454e-02,  5.9936e-03, -9.6483e-02,  1.6365e-02, -2.9208e-02,\n",
      "          7.3808e-03, -2.3459e-02, -1.7149e-03,  3.4773e-02, -9.6461e-02,\n",
      "         -7.3800e-03, -3.5344e-02,  1.2829e-02, -2.5891e-02, -3.6639e-03,\n",
      "         -9.6824e-03, -2.6698e-03, -3.4022e-02,  5.6424e-03, -3.4555e-02,\n",
      "         -1.7571e-02,  8.5784e-02, -9.2519e-03,  3.8166e-02,  3.9606e-02,\n",
      "          4.8644e-02, -4.5280e-02, -1.3929e-02, -3.7649e-02,  3.7989e-02,\n",
      "         -1.6019e-02, -5.5649e-02, -7.9676e-03,  3.4925e-02, -7.3329e-02,\n",
      "         -8.6057e-03, -9.7053e-02,  1.3548e-02,  2.0751e-02, -8.8682e-04,\n",
      "         -9.8724e-03, -8.3146e-03, -5.5299e-02, -7.9559e-02, -4.4164e-02,\n",
      "          8.8412e-03,  5.1261e-03,  3.6368e-03, -1.5015e-02,  1.7500e-03,\n",
      "         -1.7510e-02,  2.3312e-02, -3.9162e-02,  4.9472e-02, -2.8282e-02,\n",
      "          6.4991e-02, -4.4859e-02, -3.3196e-03, -4.5999e-02, -1.2916e-02,\n",
      "          3.4867e-03,  1.7469e-02,  2.2780e-02,  4.2644e-03, -2.2388e-02,\n",
      "          2.9130e-02, -1.7932e-02, -2.5704e-02, -3.1810e-02, -3.7146e-04,\n",
      "          5.2180e-04,  8.1317e-03, -6.6625e-03, -3.0474e-02, -4.2657e-03,\n",
      "          1.3602e-02,  3.2137e-02, -3.1063e-03, -3.9572e-03,  6.2872e-03,\n",
      "         -3.7418e-02, -7.2631e-02,  5.0060e-03,  9.6797e-03, -1.8031e-03,\n",
      "          2.3528e-02,  5.0030e-02,  5.0443e-03, -1.1170e-02,  3.4350e-02,\n",
      "          2.9897e-02,  5.6601e-03,  2.1064e-02, -1.5360e-02, -1.8394e-03,\n",
      "         -3.1715e-02, -2.2801e-02,  2.3572e-02,  1.0309e-02, -8.1703e-03,\n",
      "         -7.7315e-03,  1.1113e-02,  3.6892e-02,  2.2223e-02,  3.5389e-02,\n",
      "         -1.5670e-02,  4.0706e-02,  2.4455e-02,  1.6994e-02,  4.7739e-02,\n",
      "          1.2930e-02, -3.0270e-02,  2.1319e-02, -4.6599e-02, -1.2786e-02,\n",
      "          6.3610e-02,  2.0423e-02, -6.1331e-03,  2.1140e-02,  8.4780e-03,\n",
      "         -2.9949e-03, -3.8388e-02,  4.9020e-04,  2.2331e-02, -9.4532e-03,\n",
      "          1.4944e-02, -1.4554e-02,  2.1142e-02, -4.6490e-02, -1.1110e-02,\n",
      "          9.7119e-03,  6.8762e-03, -3.3349e-02,  7.7282e-03, -3.5796e-03,\n",
      "         -3.0264e-02,  6.5594e-03,  2.0009e-02, -9.2725e-03,  9.4697e-03,\n",
      "         -2.4684e-02, -9.4503e-03,  5.7975e-02, -4.4516e-02, -2.4488e-02,\n",
      "         -2.7190e-02, -1.1889e-02,  3.4450e-02, -3.1152e-02, -1.9562e-02,\n",
      "          4.1544e-02, -1.3281e-02, -7.8339e-03, -3.7543e-02,  1.2466e-02,\n",
      "          1.6454e-02, -3.2252e-03]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Load your image\n",
    "image = Image.open(\"Untitled.jpg\")  # Replace with your image path\n",
    "\n",
    "# Process the image for the model\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Get image embeddings\n",
    "with torch.no_grad():\n",
    "    image_embeddings = model.get_image_features(inputs[\"pixel_values\"])\n",
    "\n",
    "# Normalize embeddings if required\n",
    "normalized_image_embeddings = image_embeddings / image_embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# Print or use the image embeddings\n",
    "print(\"Image Embeddings:\", normalized_image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5544e-01,  5.9084e-02,  3.7951e-02, -6.3382e-02,  9.7014e-02,\n",
       "         -7.8946e-01,  2.4868e-01,  4.1952e-01, -2.9752e-01, -9.2192e-02,\n",
       "         -3.7589e-01, -1.6071e-01,  3.9774e-01,  4.5189e-01,  6.3496e-01,\n",
       "          1.7193e-01,  5.4668e-01, -1.6158e-01,  1.9366e-01, -1.3388e-01,\n",
       "         -6.0702e-01,  3.9764e-02,  2.0660e-01, -5.0958e-01, -3.1608e-01,\n",
       "         -4.8407e-02,  3.4101e-01,  4.6389e-02,  1.1899e-01, -1.7241e-01,\n",
       "          1.7805e-01,  1.4163e-01, -3.1932e-01,  5.1344e-03,  9.2927e-02,\n",
       "          5.0832e-01,  3.0952e-01,  2.8247e-01, -2.6204e-01,  6.4464e-01,\n",
       "         -5.8918e-01,  2.5444e-01, -1.3655e-01, -5.4368e-02,  2.7041e-01,\n",
       "          6.1043e-01, -2.8882e-01, -2.1235e-02,  1.8805e-01,  2.4318e-01,\n",
       "          1.3256e-01,  3.6551e-01, -1.0664e-01, -3.5088e-01,  4.4812e-01,\n",
       "          2.5063e-02, -6.3491e-02,  2.2419e-02, -2.1105e-01, -1.9775e-02,\n",
       "          1.5188e+00,  2.2652e-01,  3.3502e-01,  2.4210e-01, -2.6692e-01,\n",
       "         -2.5169e-01, -2.2722e-01,  5.0508e-01,  5.8427e-02,  2.3866e-02,\n",
       "         -1.8689e-01,  2.1508e-02, -1.8671e-01, -1.1873e-02, -7.7190e-01,\n",
       "         -1.2129e-01,  2.9334e-02, -1.9701e-01,  1.4013e-01, -5.8766e-02,\n",
       "          2.0459e-01, -1.4887e-01, -9.5945e-02, -5.8276e-01,  5.2866e-01,\n",
       "         -1.1148e-01,  2.5581e-01, -2.9105e-01,  2.3725e-01, -6.4900e-02,\n",
       "         -2.8021e-01, -2.0778e-01, -7.8845e+00, -1.7992e-01, -2.3766e-01,\n",
       "         -3.2772e-02, -1.5200e-02,  2.1619e-01, -1.3682e+00,  8.2109e-03,\n",
       "          1.2206e-01, -7.5472e-02,  2.9682e-01,  2.4093e-01,  5.3361e-02,\n",
       "          6.4606e-02,  1.6128e+00,  4.0689e-01,  2.0060e-01, -7.1224e-02,\n",
       "          3.3191e-01,  4.7014e-01,  3.9939e-01, -1.3097e-01,  3.0741e-02,\n",
       "          3.5990e-01, -1.4740e-01, -7.3659e-02, -5.3318e-02, -2.1516e-01,\n",
       "          2.5963e-01,  9.1817e-01,  3.6353e-01, -3.2341e-01,  3.8498e-02,\n",
       "         -3.2254e-01, -2.2997e-02,  3.0926e-01, -8.8019e-02, -5.1452e-01,\n",
       "          2.2369e-01,  4.0028e-01, -3.9757e-01,  1.0011e+00, -5.9035e-02,\n",
       "         -4.9969e-02, -1.0134e-01, -6.8151e-01,  2.8275e-01, -2.6449e-01,\n",
       "          1.2965e-01,  1.1473e-02, -7.0430e-01,  3.5280e-01, -8.1087e-01,\n",
       "          6.9724e-01, -2.1509e-02,  3.0570e-01, -2.4737e-01,  2.5904e-01,\n",
       "         -7.9629e-02,  2.8503e-01,  1.9591e-01, -1.6709e-01, -5.7854e-01,\n",
       "          2.8948e-01, -4.9330e-01, -4.8214e-01, -5.3897e-01,  1.7479e-01,\n",
       "         -8.2543e-02,  3.2339e-01, -1.7330e-01, -3.3973e-02,  7.5375e-01,\n",
       "         -7.6603e-03, -3.0903e-01, -9.4324e-02, -1.4995e-01,  4.6970e-01,\n",
       "         -1.6183e-01,  5.9889e-02,  2.8940e-01, -1.8300e-01,  2.5878e-01,\n",
       "          2.7129e-01,  1.5135e-02, -4.4908e-01,  5.7938e-01, -3.4032e-02,\n",
       "         -3.5481e-01,  2.4652e-01,  2.0003e-01, -2.6876e-01,  1.2299e-01,\n",
       "         -1.1945e-01, -5.9324e-01,  2.8422e-01,  1.4382e-02,  5.1206e-02,\n",
       "          1.8281e-01,  2.0501e-01,  3.4089e-01, -3.0004e-01, -5.7752e-01,\n",
       "          2.3299e-01,  1.5572e-01, -6.4973e-02, -8.7993e-01, -2.5725e-02,\n",
       "          2.6916e-02, -2.3553e-01,  2.4966e-01,  2.5683e-01, -4.3202e-01,\n",
       "         -2.9654e-01,  3.3036e-02, -3.6977e-01, -6.0175e-02,  4.2561e-01,\n",
       "          3.2528e-01,  1.4759e-01,  1.3190e-01,  3.9345e-01, -3.2645e-01,\n",
       "          2.3822e-02,  6.5834e-02,  3.1456e-01,  1.6949e+00, -6.6068e-02,\n",
       "          8.9821e-01,  1.9774e-01,  4.8110e-01,  3.6370e-01, -4.3420e-01,\n",
       "         -2.3953e-01,  1.3954e-01, -1.8140e-01, -6.3987e-02, -2.1549e-01,\n",
       "          1.7873e-01,  2.8668e-01,  1.8141e-01,  8.4620e-01,  1.8487e-01,\n",
       "         -3.2878e-01, -1.7580e-01, -5.6341e-01,  6.9230e-01, -1.8840e-01,\n",
       "          7.0921e-02,  3.7284e-01,  1.1590e-01, -6.3525e-02,  4.4387e-01,\n",
       "          4.5539e-01,  7.7998e-02, -7.3866e-01,  4.3452e-01,  1.0151e-01,\n",
       "          3.8448e-01, -5.1658e-02,  2.6761e-01, -4.3483e-01,  4.3178e-02,\n",
       "          1.5147e-01,  7.1172e-02,  2.6489e-01,  7.7258e-02, -1.6588e-01,\n",
       "         -3.3337e-01,  1.9166e-01,  3.4050e-01, -1.7801e-01, -2.2656e-01,\n",
       "          6.6188e-03, -3.4784e-01, -2.8230e-01, -4.2088e-01,  5.1811e-01,\n",
       "         -9.8154e-03,  6.6670e-02,  3.1599e-01,  1.9344e-01, -2.4261e-01,\n",
       "         -1.0551e-01,  1.8948e-01,  3.8000e-01, -1.3577e-02,  2.1654e-01,\n",
       "         -2.8270e-01, -3.0961e-02, -4.1031e-01,  4.2020e-02, -5.6370e-02,\n",
       "         -1.4092e-01, -6.7751e-01, -3.0721e-01,  7.2686e-02,  7.8296e-02,\n",
       "          6.5788e-02,  2.9050e-01,  5.6921e-02,  2.8109e-01, -4.2635e-02,\n",
       "          2.5506e-01, -9.1675e-02, -1.2571e-02,  2.8885e-01, -4.3358e-01,\n",
       "         -5.9909e-03, -1.1140e-01, -5.7531e-01, -1.5740e-01,  1.9239e-01,\n",
       "          3.4673e-01, -5.2520e-02,  2.5830e-01,  1.6640e-01,  4.1381e-01,\n",
       "         -3.6364e-02,  1.5269e-01,  9.9999e-01, -3.3672e-01, -5.4127e-02,\n",
       "         -3.9783e-01,  1.8120e-01,  3.3931e-01, -5.7594e-01, -6.3393e-01,\n",
       "          5.7940e-01,  7.9554e-01, -1.2214e-01,  1.3048e-01,  2.8841e-01,\n",
       "          1.5211e-01, -2.5999e-01,  2.5921e-01,  1.3322e-01,  3.9276e-01,\n",
       "         -9.7929e-02, -4.7953e-01,  1.2972e-01, -2.1783e-01,  1.0565e-01,\n",
       "          2.6060e-02,  5.1448e-01,  2.3409e-01, -2.8449e-01, -6.7694e-01,\n",
       "         -1.0497e-01, -1.2882e-01, -1.6533e-01, -7.1456e-02,  1.2108e-01,\n",
       "          3.7565e-01,  2.0216e-01,  1.4309e-01, -8.7040e-02, -3.0578e-01,\n",
       "         -9.2180e-01,  2.8930e-01,  3.7697e-01,  1.3473e-01,  1.4814e-01,\n",
       "          6.6415e-01,  6.8099e-02, -1.0962e+00,  1.8594e-01, -3.3186e-01,\n",
       "          8.3860e-02, -2.6654e-01, -1.9485e-02,  3.9508e-01, -1.0960e+00,\n",
       "         -8.3851e-02, -4.0157e-01,  1.4576e-01, -2.9417e-01, -4.1629e-02,\n",
       "         -1.1001e-01, -3.0334e-02, -3.8656e-01,  6.4109e-02, -3.9261e-01,\n",
       "         -1.9964e-01,  9.7468e-01, -1.0512e-01,  4.3364e-01,  4.5000e-01,\n",
       "          5.5270e-01, -5.1447e-01, -1.5826e-01, -4.2776e-01,  4.3162e-01,\n",
       "         -1.8201e-01, -6.3228e-01, -9.0527e-02,  3.9681e-01, -8.3316e-01,\n",
       "         -9.7778e-02, -1.1027e+00,  1.5393e-01,  2.3578e-01, -1.0076e-02,\n",
       "         -1.1217e-01, -9.4470e-02, -6.2831e-01, -9.0395e-01, -5.0178e-01,\n",
       "          1.0045e-01,  5.8243e-02,  4.1322e-02, -1.7060e-01,  1.9883e-02,\n",
       "         -1.9895e-01,  2.6487e-01, -4.4495e-01,  5.6210e-01, -3.2134e-01,\n",
       "          7.3842e-01, -5.0969e-01, -3.7717e-02, -5.2264e-01, -1.4675e-01,\n",
       "          3.9616e-02,  1.9849e-01,  2.5882e-01,  4.8451e-02, -2.5437e-01,\n",
       "          3.3098e-01, -2.0374e-01, -2.9205e-01, -3.6142e-01, -4.2205e-03,\n",
       "          5.9287e-03,  9.2391e-02, -7.5699e-02, -3.4624e-01, -4.8467e-02,\n",
       "          1.5454e-01,  3.6514e-01, -3.5294e-02, -4.4961e-02,  7.1435e-02,\n",
       "         -4.2514e-01, -8.2523e-01,  5.6878e-02,  1.0998e-01, -2.0486e-02,\n",
       "          2.6732e-01,  5.6843e-01,  5.7313e-02, -1.2691e-01,  3.9028e-01,\n",
       "          3.3969e-01,  6.4310e-02,  2.3933e-01, -1.7451e-01, -2.0899e-02,\n",
       "         -3.6035e-01, -2.5907e-01,  2.6782e-01,  1.1714e-01, -9.2831e-02,\n",
       "         -8.7845e-02,  1.2626e-01,  4.1916e-01,  2.5250e-01,  4.0209e-01,\n",
       "         -1.7804e-01,  4.6250e-01,  2.7786e-01,  1.9308e-01,  5.4241e-01,\n",
       "          1.4691e-01, -3.4393e-01,  2.4222e-01, -5.2945e-01, -1.4528e-01,\n",
       "          7.2273e-01,  2.3205e-01, -6.9684e-02,  2.4019e-01,  9.6326e-02,\n",
       "         -3.4028e-02, -4.3616e-01,  5.5696e-03,  2.5372e-01, -1.0741e-01,\n",
       "          1.6979e-01, -1.6536e-01,  2.4022e-01, -5.2822e-01, -1.2624e-01,\n",
       "          1.1035e-01,  7.8127e-02, -3.7891e-01,  8.7807e-02, -4.0671e-02,\n",
       "         -3.4386e-01,  7.4527e-02,  2.2734e-01, -1.0535e-01,  1.0759e-01,\n",
       "         -2.8046e-01, -1.0737e-01,  6.5870e-01, -5.0579e-01, -2.7823e-01,\n",
       "         -3.0893e-01, -1.3509e-01,  3.9142e-01, -3.5394e-01, -2.2227e-01,\n",
       "          4.7202e-01, -1.5090e-01, -8.9008e-02, -4.2656e-01,  1.4164e-01,\n",
       "          1.8695e-01, -3.6645e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ip_adapter import IPAdapterXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HFModelHub' from 'transformers' (/home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m HFModelHub\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m HFModelHub\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mh94/IP-Adapter\u001b[39m\u001b[39m\"\u001b[39m, subfolder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msdxl_models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m\"\u001b[39m\u001b[39mpath/to/IP-Adapter\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HFModelHub' from 'transformers' (/home/ptummal3/.conda/envs/clip/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import HFModelHub\n",
    "model = HFModelHub.from_pretrained(\"h94/IP-Adapter\", subfolder=\"sdxl_models\")\n",
    "model.save_pretrained(\"path/to/IP-Adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ipadapter_text2image(self, text, image=None):\n",
    "        self.text2img.load_ip_adapter(\n",
    "            \"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\"\n",
    "        )\n",
    "        self.text2img.to(self.device)\n",
    "        image = self.text2img(\n",
    "            prompt=text, ip_adapter_image=image, num_inference_steps=50\n",
    "        ).images[0]\n",
    "        self.text2img.to(\"cpu\")\n",
    "        self.text2img.unload_ip_adapter()\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  6.66it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLaughing face of a girl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m utils \u001b[39m=\u001b[39m Utils(device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m image\u001b[39m=\u001b[39mutils\u001b[39m.\u001b[39;49mtext2image(text)\n\u001b[1;32m      5\u001b[0m image\n",
      "File \u001b[0;32m~/Downloads/clip/utils.py:63\u001b[0m, in \u001b[0;36mUtils.text2image\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtext2image\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[1;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext2img\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 63\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext2img(text, num_inference_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext2img\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:1020\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m latent_model_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[1;32m   1019\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[0;32m-> 1020\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(\n\u001b[1;32m   1021\u001b[0m     latent_model_input,\n\u001b[1;32m   1022\u001b[0m     t,\n\u001b[1;32m   1023\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mprompt_embeds,\n\u001b[1;32m   1024\u001b[0m     timestep_cond\u001b[39m=\u001b[39;49mtimestep_cond,\n\u001b[1;32m   1025\u001b[0m     cross_attention_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m   1026\u001b[0m     added_cond_kwargs\u001b[39m=\u001b[39;49madded_cond_kwargs,\n\u001b[1;32m   1027\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1028\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py:1152\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         emb \u001b[39m=\u001b[39m emb \u001b[39m+\u001b[39m class_emb\n\u001b[0;32m-> 1152\u001b[0m aug_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_aug_embed(\n\u001b[1;32m   1153\u001b[0m     emb\u001b[39m=\u001b[39;49memb, encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states, added_cond_kwargs\u001b[39m=\u001b[39;49madded_cond_kwargs\n\u001b[1;32m   1154\u001b[0m )\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39maddition_embed_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mimage_hint\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     aug_emb, hint \u001b[39m=\u001b[39m aug_emb\n",
      "File \u001b[0;32m~/.conda/envs/clip/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py:970\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_aug_embed\u001b[0;34m(self, emb, encoder_hidden_states, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    967\u001b[0m     aug_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_embedding(text_embs, image_embs)\n\u001b[1;32m    968\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39maddition_embed_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtext_time\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    969\u001b[0m     \u001b[39m# SDXL - style\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39;49m\u001b[39mtext_embeds\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m added_cond_kwargs:\n\u001b[1;32m    971\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    972\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has the config param `addition_embed_type` set to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext_time\u001b[39m\u001b[39m'\u001b[39m\u001b[39m which requires the keyword argument `text_embeds` to be passed in `added_cond_kwargs`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m     text_embeds \u001b[39m=\u001b[39m added_cond_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "from utils import Utils\n",
    "text=\"Laughing face of a girl\"\n",
    "utils = Utils(device=\"cuda\")\n",
    "image=utils.text2image(text)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  7.87it/s]\n",
      "100%|██████████| 50/50 [00:04<00:00, 10.11it/s]\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    }
   ],
   "source": [
    "from utils import Utils\n",
    "from diffusers import DiffusionPipeline\n",
    "utils_class = Utils()\n",
    "\n",
    "text_prompt =\"single dog, hat, red, polka dot, dog wearing hat\"\n",
    "output = utils_class.text2image(text_prompt)\n",
    "output.show()\n",
    "\n",
    "# text2img: DiffusionPipeline = (\n",
    "#     DiffusionPipeline.from_pretrained(\n",
    "#         \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#         use_safetensors=True,\n",
    "#         variant=\"fp16\",\n",
    "#         )\n",
    "# )\n",
    "\n",
    "# text2img.to(\"cuda\")\n",
    "# image = text2img(text_prompt, num_inference_steps=50).images[0]\n",
    "# text2img.to(\"cpu\")\n",
    "\n",
    "# image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  7.93it/s]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.65it/s]\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    }
   ],
   "source": [
    "from utils import Utils\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the Utils class, setting device to 'cuda' for GPU\n",
    "utils = Utils(device=\"cuda\")\n",
    "\n",
    "# Load the image from file\n",
    "image_path = \"dog3.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define a sample text prompt\n",
    "text_prompt = \"cute red hat with white dots pattern\"\n",
    "\n",
    "# Generate an image using IP-Adapter with the text prompt\n",
    "generated_image = utils.ipadapter_text2image(text=text_prompt,image=image)\n",
    "\n",
    "# Display the generated image\n",
    "generated_image.show()  # This opens the image in a default viewer\n",
    "\n",
    "# Optionally, save the generated image\n",
    "generated_image.save(\"ip_adapter_generated_image.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
